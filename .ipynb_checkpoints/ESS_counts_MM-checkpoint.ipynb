{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddef6162-bb57-4fd0-a574-0715b3df9208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, pearsonr\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import accuracy_score,  cohen_kappa_score\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5060be66-1f8b-4af9-9b22-721da8dbd261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_agreement(array1, array2):\n",
    "    exact_agreement = 0\n",
    "    adjacent_agreement = 0\n",
    "    discrepancy = 0\n",
    "\n",
    "    n = len(array1)\n",
    "    for i in range(n - 1):\n",
    "        # Exact Agreement\n",
    "        if array1[i] == array2[i]:\n",
    "            exact_agreement += 1\n",
    "\n",
    "        # Adjacent Agreement\n",
    "        if abs(array1[i] - array2[i]) == 1:\n",
    "            adjacent_agreement += 1\n",
    "\n",
    "        # Discrepancy\n",
    "        if abs(array1[i] - array2[i])>=2:\n",
    "            discrepancy += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    exact_agreement_pct = exact_agreement / n * 100\n",
    "    adjacent_agreement_pct = adjacent_agreement / n * 100\n",
    "    discrepancy_pct = discrepancy / n * 100\n",
    "    \n",
    "    # Compute Linear Weighted Kappa\n",
    "    linear_weighted_kappa = cohen_kappa_score(array1, array2, weights='linear')\n",
    "    quadratic_weighted_kappa = cohen_kappa_score(array1, array2, weights='quadratic')\n",
    "\n",
    "    return exact_agreement_pct, adjacent_agreement_pct, discrepancy_pct, linear_weighted_kappa, quadratic_weighted_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5f8305-bb27-406c-90f8-a9d533ec962f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_discrete_data_agreement(rho,n,ptile1,ptile2,ptile3):\n",
    "    mean = (0, 0)\n",
    "    cov = [[1, rho], [rho, 1]]\n",
    "    v = np.random.multivariate_normal(mean, cov, (n))\n",
    "    x = v[:,0]\n",
    "    y = v[:,1]\n",
    "    x_cut1 = np.percentile(x,ptile1)\n",
    "    x_cut2 = np.percentile(x,ptile2)\n",
    "    x_cut3 = np.percentile(x,ptile3)\n",
    "    x[x < x_cut1]=1\n",
    "    x[(x >= x_cut1)*(x < x_cut2)*(x !=1)] = 2\n",
    "    x[(x >= x_cut2)*(x < x_cut3)*(x !=1)*(x !=2)] = 3\n",
    "    x[(x >= x_cut3)*(x !=1)*(x !=2)*(x!=3)] = 4\n",
    "    \n",
    "    y_cut1 = np.percentile(y,ptile1)\n",
    "    y_cut2 = np.percentile(y,ptile2)\n",
    "    y_cut3 = np.percentile(y,ptile3)\n",
    "    \n",
    "    z = np.empty(n)\n",
    "    z[y < y_cut1] = 1\n",
    "    z[(y >= y_cut1)*(y < y_cut2)*(y !=1)] = 2\n",
    "    z[(y >= y_cut2)*(y < y_cut3)*(y !=1)*(y !=2)] = 3\n",
    "    z[(y >= y_cut3)*(y !=1)*(y !=2)*(y!=3)] = 4\n",
    "    \n",
    "    #rho_out = pearsonr(x,y)[0] \n",
    "    # df = pd.DataFrame([x,y]).T\n",
    "    # df.columns=['ALDs','difficulty_parameter']\n",
    "    # fig = px.scatter(df,x='ALDs',y='difficulty_parameter',marginal_y='histogram',color='ALDs'\\\n",
    "    #                ,trendline='ols',title='Correlation: '+str(rho_out)[0:4])\n",
    "    # fig.update_layout(width=800, height=500)\n",
    "    # fig.show()\n",
    "    \n",
    "    df = pd.DataFrame([x,y]).T\n",
    "    df.columns=['ALDs','difficulty_parameter']\n",
    "    exact_pct, adjacent_pct, discrepancy_pct, linear_kappa, quadratic_kappa = compute_agreement(x, z)\n",
    "    return (df, exact_pct, adjacent_pct, discrepancy_pct, linear_kappa, quadratic_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5577daa6-da7d-402a-84f4-593fd090cd63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_discrete_data_agreement(0.7,1000,25,50,75)[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3d236-c0c7-4eaa-94d5-18731cf1914f",
   "metadata": {},
   "source": [
    "# 1. Correlational Upper Bound under Perfect Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e22ae56-37a5-489f-8679-b41e29cea7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'P1': [25, 15, 30, 5],\n",
    "    'P2': [50, 50, 60, 20],\n",
    "    'P3': [75, 85, 90, 70]\n",
    "}\n",
    "\n",
    "dis = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636eb188-76ea-48ab-9498-0940d035866c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1  P2  P3\n",
       "0  25  50  75\n",
       "1  15  50  85\n",
       "2  30  60  90\n",
       "3   5  20  70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8711699-09e6-49d9-9483-92c3770f1c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_cor_table = []\n",
    "dist_names = ['uniform','non-uniform-bal','skewed-A','skewed-B']\n",
    "for i, dist_name in enumerate (dist_names): \n",
    "    for j in range(10):\n",
    "        np.random.seed(j)\n",
    "        df = generate_discrete_data_agreement(1, 1000, dis['P1'][i], dis['P2'][i],dis['P3'][i])[0]\n",
    "        correlation = df.corr().iloc[0,1]\n",
    "        per_cor_table.append((dist_name, j, correlation))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7a9010-a937-46ec-b8ca-941563377362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distribution\n",
       "non-uniform-bal    0.94\n",
       "skewed-A           0.93\n",
       "skewed-B           0.91\n",
       "uniform            0.92\n",
       "Name: correlation, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_cor_table = pd.DataFrame(per_cor_table).rename(columns = {0:'distribution',1:'iter',2:'correlation'})\n",
    "round(per_cor_table.groupby('distribution').correlation.mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b817b2a-99d1-4222-9286-9b9a728f6e78",
   "metadata": {},
   "source": [
    "# 2. ESS Count Based Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65816d3c-e99a-4a48-87c7-d4cd8fd9bd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimal_cut (data,ptile,level_number):# ptile is the percentile for the distribution,\n",
    "    # e.g. ptile = 25 for the first cut of the uniform distribution\n",
    "    # e.g. level_number = '12' for the first cut of the distribution\n",
    "    if ptile - 10 < 0:\n",
    "        ptile_minus10 = 0\n",
    "    else:\n",
    "        ptile_minus10 = ptile - 10\n",
    "    cut_range_low = round(np.percentile(data.difficulty_parameter,ptile_minus10),1)  # Starting number\n",
    "\n",
    "    if ptile + 10 > 100:\n",
    "        ptile_add10 = 100\n",
    "    else:\n",
    "        ptile_add10 = ptile + 10\n",
    "    cut_range_high = round(np.percentile(data.difficulty_parameter,ptile_add10),1)  # Ending number\n",
    "    \n",
    "    accuracy_table = []\n",
    "    #----------------------\n",
    "    step_size = 0.01  # Desired step size between values\n",
    "    cut_values = np.arange(cut_range_low, cut_range_high, step_size)\n",
    "    cut_values = np.round(cut_values,2)\n",
    "    for cut in cut_values:\n",
    "        data.loc[data.difficulty_parameter < cut,'level'+level_number] = 'low'\n",
    "        data.loc[data.difficulty_parameter >= cut,'level'+level_number] = 'high'\n",
    "        accuracy_table.append((cut,accuracy_score(data['ALD'+level_number], data['level'+level_number])))\n",
    "    accuracy_table = pd.DataFrame(accuracy_table).rename(columns = {0:'Cut',1:'accuracy'}) # accuracy table for all cuts\n",
    "    accuracy_max = accuracy_table.loc[accuracy_table.accuracy == accuracy_table.accuracy.max(),:] # find out the max accuracy\n",
    "    optimal_cutoff = round(accuracy_max.Cut.mean(),2)\n",
    "    return (optimal_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa04bea9-5aba-4ac7-9912-55f332ae70e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "method = 'ESS_Counts'\n",
    "def cal_agreement(rho,n,ptile1,ptile2,ptile3):\n",
    "    cut_table = []\n",
    "    agreement_table = []\n",
    "    percent_table = []\n",
    "    for i in range(10): \n",
    "        np.random.seed(i)\n",
    "        df = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[0] # change as correlation level change\n",
    "        true_exa = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[1]\n",
    "        true_adj = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[2]\n",
    "        true_dis = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[3]\n",
    "        true_lkap = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[4]\n",
    "        true_qkap = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[5]\n",
    "        \n",
    "        df.loc[df.ALDs ==1, 'ALD12'] = 'low' # recode ALDs to low or high for each ALD cut\n",
    "        df.loc[df.ALDs > 1, 'ALD12'] = 'high'\n",
    "        df.loc[df.ALDs <=2, 'ALD23'] = 'low'\n",
    "        df.loc[df.ALDs > 2, 'ALD23'] = 'high'\n",
    "        df.loc[df.ALDs <=3, 'ALD34'] = 'low'\n",
    "        df.loc[df.ALDs > 3, 'ALD34'] = 'high'\n",
    "        #------------------\n",
    "        ptile_cut1 = round(np.percentile(df.difficulty_parameter,ptile1),2)\n",
    "        ptile_cut2 = round(np.percentile(df.difficulty_parameter,ptile2),2)\n",
    "        ptile_cut3 = round(np.percentile(df.difficulty_parameter,ptile3),2)\n",
    "        #print(ptile_cut1,ptile_cut2,ptile_cut3)\n",
    "        #----------------------------------\n",
    "        cut1 = optimal_cut(df,ptile1,'12') # change as number of categories change\n",
    "        cut2 = optimal_cut(df,ptile2,'23') # change as number of categories change\n",
    "        cut3 = optimal_cut(df,ptile3,'34') # change as number of categories change\n",
    "\n",
    "        #---------------------------\n",
    "        df.loc[(df.difficulty_parameter<cut1),'ALDs_pred'] = 1\n",
    "        df.loc[(df.difficulty_parameter>=cut1) & (df.difficulty_parameter<cut2),'ALDs_pred'] = 2\n",
    "        df.loc[(df.difficulty_parameter>=cut2) & (df.difficulty_parameter<cut3),'ALDs_pred'] = 3\n",
    "        df.loc[(df.difficulty_parameter>=cut3),'ALDs_pred'] = 4\n",
    "        \n",
    "        #---generate crosstab tables------------------\n",
    "        crosstab_table = (pd.crosstab(df['ALDs'],df['ALDs_pred'],margins=True, margins_name='Total')/10).reset_index()\n",
    "\n",
    "        # Calculate exact agreement\n",
    "        exact_agreement = round(len(df.loc[df.ALDs == df.ALDs_pred,:])/ len(df)*100,1)\n",
    "\n",
    "        # Calculate adjacent agreement\n",
    "        adjacent_agreement = round(np.sum(np.abs(df['ALDs'] - df['ALDs_pred']) == 1) / len(df)*100,1)\n",
    "\n",
    "        # Calculate discrepant \n",
    "        discrepant = round(np.sum(np.abs(df['ALDs'] - df['ALDs_pred']) > 1)/len(df)*100,1)\n",
    "\n",
    "        # Calculate linear-weighted quadratic kappa\n",
    "        Lkappa = round(cohen_kappa_score(df['ALDs'], df['ALDs_pred'], weights='linear'),3)\n",
    "        Qkappa = round(cohen_kappa_score(df['ALDs'], df['ALDs_pred'], weights='quadratic'),3)\n",
    "        \n",
    "\n",
    "        # correlation\n",
    "        r = round(spearmanr(df['ALDs'], df['difficulty_parameter'])[0],3)\n",
    "        n1 = round(len(df.loc[df.ALDs_pred==1,:])/1000*100,1)\n",
    "        n2 = round(len(df.loc[df.ALDs_pred==2,:])/1000*100,1)\n",
    "        n3 = round(len(df.loc[df.ALDs_pred==3,:])/1000*100,1)\n",
    "        n4 = round(len(df.loc[df.ALDs_pred==4,:])/1000*100,1)\n",
    "\n",
    "        cut_table.append((rho,r, ptile1,ptile2,ptile3,i, ptile_cut1, ptile_cut2, ptile_cut3, cut1, cut2, cut3))\n",
    "        agreement_table.append((rho,r, ptile1,ptile2,ptile3,i,exact_agreement,true_exa, exact_agreement-true_exa,\\\n",
    "                                adjacent_agreement,true_adj,adjacent_agreement-true_adj,\\\n",
    "                                discrepant,true_dis, discrepant-true_dis,\\\n",
    "                                Lkappa,true_lkap,Lkappa-true_lkap,\\\n",
    "                                Qkappa,true_qkap,Qkappa-true_qkap))\n",
    "        percent_table.append((rho,r,i, ptile1, ptile2-ptile1,ptile3-ptile2,100-ptile3,n1,n2,n3,n4))\n",
    "        \n",
    "    cut_table = pd.DataFrame(cut_table).rename(columns = {0:'rho',1:'correlation',2:'ptile1',3:'ptile2',4:'ptile3',\\\n",
    "                                                  5:'iter',6:'true_cut1', 7:'true_cut2',8: 'true_cut3',9:'cut1',\\\n",
    "                                                          10:'cut2',11: 'cut3'})\n",
    "    cut_table['Dif_cut1'] = cut_table['cut1'] - cut_table['true_cut1']\n",
    "    cut_table['Dif_cut2'] = cut_table['cut2'] - cut_table['true_cut2']\n",
    "    cut_table['Dif_cut3'] = cut_table['cut3'] - cut_table['true_cut3']\n",
    "    cut_table = cut_table[['rho','correlation','ptile1','ptile2','ptile3','iter','true_cut1','true_cut2','true_cut3',\\\n",
    "                           'cut1','cut2','cut3','Dif_cut1','Dif_cut2','Dif_cut3']]\n",
    "    \n",
    "    agreement_table = pd.DataFrame(agreement_table).rename(columns = {0:'rho',1:'correlation',2:'ptile1',3:'ptile2',\\\n",
    "                                                                      4:'ptile3',5:'iter',6:'exa',7:'true_exa', 8:'dif_exa',\\\n",
    "                                                                      9:'adj',10:'true_adj',11:'dif_adj',\\\n",
    "                                                                      12:'dis',13:'true_dis',14:'dif_dis',\\\n",
    "                                                                      15:'lkappa',16:'true_lkappa',17:'dif_lk',\\\n",
    "                                                                      18:'qkappa',19:'true_qkappa',20:'dif_qk'})\n",
    "    percent_table = pd.DataFrame(percent_table).rename(columns = {0:'rho',1:'correlation',2:'iter',3:'per_L1',4:'per_L2',\\\n",
    "                                                                      5:'per_L3',6:'per_L4',7:'L1',8:'L2',\\\n",
    "                                                                  9:'L3',10:'L4'})\n",
    "\n",
    "    return (cut_table, agreement_table, percent_table, crosstab_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b15bc9-e2b7-45cd-bc84-25836a31cd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cal_agreement(0.8,1000,25,50,75)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cfbcf-dc20-4849-becf-d879c67470e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Mean and Midpoint Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6c95fb-8c9b-4d0d-96d3-e016019b297e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "method = 'ESS_Counts_Mean'# Mean Score of Each Score Category, then Midpoint of the Mean Scores \n",
    "#(e.g., (mean1+mean2)/2) is the cut\n",
    "def cal_agreement(rho,n,ptile1,ptile2,ptile3):\n",
    "    cut_table = []\n",
    "    agreement_table = []\n",
    "    percent_table = []\n",
    "    for i in range(10):\n",
    "        np.random.seed(i)\n",
    "        df = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[0] # change as correlation level change\n",
    "        true_exa = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[1]\n",
    "        true_adj = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[2]\n",
    "        true_dis = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[3]\n",
    "        true_lkap = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[4]\n",
    "        true_qkap = generate_discrete_data_agreement(rho, n, ptile1, ptile2, ptile3)[5]\n",
    "        \n",
    "        #------------------\n",
    "        ptile_cut1 = round(np.percentile(df.difficulty_parameter,ptile1),2)\n",
    "        ptile_cut2 = round(np.percentile(df.difficulty_parameter,ptile2),2)\n",
    "        ptile_cut3 = round(np.percentile(df.difficulty_parameter,ptile3),2)\n",
    "        #print(ptile_cut1,ptile_cut2,ptile_cut3)\n",
    "        #----------------------------------\n",
    "        mean1 = df.loc[df.ALDs == 1,'difficulty_parameter'].mean() # change as number of categories change\n",
    "        mean2 = df.loc[df.ALDs == 2,'difficulty_parameter'].mean()# change as number of categories change\n",
    "        mean3 = df.loc[df.ALDs == 3,'difficulty_parameter'].mean()# change as number of categories change\n",
    "        mean4 = df.loc[df.ALDs == 4,'difficulty_parameter'].mean() # change as number of categories change\n",
    "\n",
    "        cut1 = round((mean1+mean2)/2,2)\n",
    "        cut2 = round((mean2+mean3)/2,2)\n",
    "        cut3 = round((mean3+mean4)/2,2)\n",
    "\n",
    "        #---------------------------\n",
    "        df.loc[(df.difficulty_parameter<cut1),'ALDs_pred'] = 1\n",
    "        df.loc[(df.difficulty_parameter>=cut1) & (df.difficulty_parameter<cut2),'ALDs_pred'] = 2\n",
    "        df.loc[(df.difficulty_parameter>=cut2) & (df.difficulty_parameter<cut3),'ALDs_pred'] = 3\n",
    "        df.loc[(df.difficulty_parameter>=cut3),'ALDs_pred'] = 4\n",
    "        \n",
    "        #---generate crosstab tables------------------\n",
    "        crosstab_table = (pd.crosstab(df['ALDs'],df['ALDs_pred'],margins=True, margins_name='Total')/10).reset_index()\n",
    "\n",
    "        # Calculate exact agreement\n",
    "        exact_agreement = round(len(df.loc[df.ALDs == df.ALDs_pred,:])/ len(df)*100,1)\n",
    "\n",
    "        # Calculate adjacent agreement\n",
    "        adjacent_agreement = round(np.sum(np.abs(df['ALDs'] - df['ALDs_pred']) == 1) / len(df)*100,1)\n",
    "\n",
    "        # Calculate discrepant \n",
    "        discrepant = round(np.sum(np.abs(df['ALDs'] - df['ALDs_pred']) > 1)/len(df)*100,1)\n",
    "\n",
    "        # Calculate linear-weighted quadratic kappa\n",
    "        Lkappa = round(cohen_kappa_score(df['ALDs'], df['ALDs_pred'], weights='linear'),3)\n",
    "        Qkappa = round(cohen_kappa_score(df['ALDs'], df['ALDs_pred'], weights='quadratic'),3)\n",
    "\n",
    "        # correlation\n",
    "        #r = round(pearsonr(df['ALDs'],df['difficulty_parameter'])[0],3)\n",
    "        r = round(spearmanr(df['ALDs'], df['difficulty_parameter'])[0],3)\n",
    "        n1 = round(len(df.loc[df.ALDs_pred==1,:])/1000*100,1)\n",
    "        n2 = round(len(df.loc[df.ALDs_pred==2,:])/1000*100,1)\n",
    "        n3 = round(len(df.loc[df.ALDs_pred==3,:])/1000*100,1)\n",
    "        n4 = round(len(df.loc[df.ALDs_pred==4,:])/1000*100,1)\n",
    "\n",
    "        cut_table.append((rho,r, ptile1,ptile2,ptile3,i, ptile_cut1, ptile_cut2, ptile_cut3, cut1, cut2, cut3))\n",
    "        agreement_table.append((rho,r, ptile1,ptile2,ptile3,i,exact_agreement,true_exa, exact_agreement-true_exa,\\\n",
    "                                adjacent_agreement,true_adj,adjacent_agreement-true_adj,\\\n",
    "                                discrepant,true_dis, discrepant-true_dis,\\\n",
    "                                Lkappa,true_lkap,Lkappa-true_lkap,\\\n",
    "                                Qkappa,true_qkap,Qkappa-true_qkap))\n",
    "        \n",
    "        percent_table.append((rho,r,i, ptile1, ptile2-ptile1,ptile3-ptile2,100-ptile3,n1,n2,n3,n4))\n",
    "        \n",
    "    cut_table = pd.DataFrame(cut_table).rename(columns = {0:'rho',1:'correlation',2:'ptile1',3:'ptile2',4:'ptile3',\\\n",
    "                                                  5:'iter',6:'true_cut1', 7:'true_cut2',8: 'true_cut3',9:'cut1',\\\n",
    "                                                          10:'cut2',11: 'cut3'})\n",
    "    cut_table['Dif_cut1'] = cut_table['cut1'] - cut_table['true_cut1']\n",
    "    cut_table['Dif_cut2'] = cut_table['cut2'] - cut_table['true_cut2']\n",
    "    cut_table['Dif_cut3'] = cut_table['cut3'] - cut_table['true_cut3']\n",
    "    cut_table = cut_table[['rho','correlation','ptile1','ptile2','ptile3','iter','true_cut1','true_cut2','true_cut3',\\\n",
    "                           'cut1','cut2','cut3','Dif_cut1','Dif_cut2','Dif_cut3']]\n",
    "    \n",
    "    agreement_table = pd.DataFrame(agreement_table).rename(columns = {0:'rho',1:'correlation',2:'ptile1',3:'ptile2',\\\n",
    "                                                                      4:'ptile3',5:'iter',6:'exa',7:'true_exa', 8:'dif_exa',\\\n",
    "                                                                      9:'adj',10:'true_adj',11:'dif_adj',\\\n",
    "                                                                      12:'dis',13:'true_dis',14:'dif_dis',\\\n",
    "                                                                      15:'lkappa',16:'true_lkappa',17:'dif_lk',\\\n",
    "                                                                      18:'qkappa',19:'true_qkappa',20:'dif_qk'})\n",
    "    percent_table = pd.DataFrame(percent_table).rename(columns = {0:'rho',1:'correlation',2:'iter',3:'per_L1',4:'per_L2',\\\n",
    "                                                                      5:'per_L3',6:'per_L4',7:'L1',8:'L2',\\\n",
    "                                                                  9:'L3',10:'L4'})\n",
    "    \n",
    "    return (cut_table, agreement_table, percent_table, crosstab_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e7ee6-6a4f-4613-a38e-1031db3b8e5c",
   "metadata": {},
   "source": [
    "# 4 Generate Results for Different Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd0efbcd-5c07-4699-af33-086569bdc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_levels = [0.3,0.55,0.80]\n",
    "data = {\n",
    "    'P1': [25, 15, 30, 5],\n",
    "    'P2': [50, 50, 60, 20],\n",
    "    'P3': [75, 85, 90, 70]\n",
    "    \n",
    "}\n",
    "\n",
    "dis = pd.DataFrame(data)\n",
    "\n",
    "cut_tables = pd.DataFrame()\n",
    "agreement_tables = pd.DataFrame()\n",
    "crosstab_tables = pd.DataFrame()\n",
    "percent_tables = pd.DataFrame()\n",
    "for cor in cor_levels:\n",
    "    if cor == 0.3:\n",
    "        cor_level_name = 'low'\n",
    "    elif cor == 0.55:\n",
    "        cor_level_name = 'medium'\n",
    "    elif cor == 0.8:\n",
    "        cor_level_name = 'high'\n",
    "        \n",
    "    for i in range(4):\n",
    "        p1 = dis['P1'][i]\n",
    "        p2 = dis['P2'][i]\n",
    "        p3 = dis['P3'][i]\n",
    "        if i == 0:\n",
    "            distribution = 'uniform'\n",
    "        elif i == 1:\n",
    "            distribution = 'non-uni-balanced'\n",
    "        elif i == 2:\n",
    "            distribution = 'skewed-A'\n",
    "        elif i == 3:\n",
    "            distribution = 'skewed-B'\n",
    "\n",
    "        cut_table = cal_agreement(cor,1000,p1,p2,p3)[0]\n",
    "        cut_table['dist'] = distribution\n",
    "        cut_table['corr'] = cor_level_name\n",
    "        cut_tables = pd.concat([cut_tables,cut_table], axis = 0)\n",
    "        #----------------\n",
    "        agreement_table = cal_agreement(cor,1000,p1,p2,p3)[1]\n",
    "        agreement_table['dist'] = distribution\n",
    "        agreement_table['corr'] = cor_level_name\n",
    "        agreement_tables = pd.concat([agreement_tables,agreement_table], axis = 0)        \n",
    "        #-------------------------------\n",
    "        percent_table = cal_agreement(cor,1000,p1,p2,p3)[2]\n",
    "        percent_table['dist'] = distribution\n",
    "        percent_table['corr'] = cor_level_name        \n",
    "        percent_tables = pd.concat([percent_tables,percent_table],axis = 0)\n",
    "        #----------------------------------\n",
    "        crosstab_table = cal_agreement(cor,1000,p1,p2,p3)[3]\n",
    "        crosstab_table['dist'] = distribution\n",
    "        crosstab_table['corr'] = cor_level_name        \n",
    "        crosstab_tables = pd.concat([crosstab_tables,crosstab_table],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e80f4-8473-4ae4-a289-701580c58d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8af4dfd1-e9af-4662-bdb2-ba8e1e8b9ce4",
   "metadata": {},
   "source": [
    "## a. Agreement Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "797b16d3-81fb-452c-94c4-c4006a7b9edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the pre-specified order\n",
    "specified_order_dist = ['uniform', 'non-uni-balanced', 'skewed-A','skewed-B']\n",
    "specified_order_corr = ['high','medium','low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "def17688-6b85-4fd9-9947-0e942811f6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agreement_table = agreement_tables.groupby(['dist','corr']).mean()\n",
    "agreement_table = agreement_table.reset_index()\n",
    "# Convert the column 'dist' to categorical with specified order\n",
    "agreement_table['dist'] = pd.Categorical(agreement_table['dist'], categories=specified_order_dist, ordered=True)\n",
    "agreement_table['corr'] = pd.Categorical(agreement_table['corr'], categories=specified_order_corr, ordered=True)\n",
    "# Sort the DataFrame by the 'dist' column\n",
    "agreement_table = agreement_table.sort_values(by=['dist','corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f58e0145-e598-426b-bd85-2e9e3c77ecd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agreement_table.round(2).to_excel(r'C:\\Users\\jichen\\Documents\\Cambium\\ESS_criteria\\2024' + '\\\\'+method+'\\\\agreement_'+method+'.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf1397-88c1-43f9-ae24-0bc6c9982bd9",
   "metadata": {},
   "source": [
    "## b. Percent Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf1f2a76-7b55-4505-9f9c-cef3e200d1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percent_table = percent_tables.groupby(['dist','corr']).mean()\n",
    "percent_table = percent_table.reset_index()\n",
    "# Convert the column 'dist' to categorical with specified order\n",
    "percent_table['dist'] = pd.Categorical(percent_table['dist'], categories=specified_order_dist, ordered=True)\n",
    "percent_table['corr'] = pd.Categorical(percent_table['corr'], categories=specified_order_corr, ordered=True)\n",
    "# Sort the DataFrame by the 'dist' column\n",
    "percent_table = percent_table.sort_values(by=['dist','corr'])\n",
    "percent_table = percent_table.round(1)\n",
    "percent_table['Dif_L1'] = abs(percent_table['per_L1']-percent_table['L1'])\n",
    "percent_table['Dif_L2'] = abs(percent_table['per_L2']-percent_table['L2']) \n",
    "percent_table['Dif_L3'] = abs(percent_table['per_L3']-percent_table['L3']) \n",
    "percent_table['Dif_L4'] = abs(percent_table['per_L4']-percent_table['L4']) \n",
    "\n",
    "percent_table.to_excel(r'C:\\Users\\jichen\\Documents\\Cambium\\ESS_criteria\\2024' + '\\\\'+method+'\\\\percentage_'+method\\\n",
    "                                +'_checking.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28592347-11d4-4a24-8595-7b03eb496bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>corr</th>\n",
       "      <th>rho</th>\n",
       "      <th>correlation</th>\n",
       "      <th>iter</th>\n",
       "      <th>per_L1</th>\n",
       "      <th>per_L2</th>\n",
       "      <th>per_L3</th>\n",
       "      <th>per_L4</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>L4</th>\n",
       "      <th>Dif_L1</th>\n",
       "      <th>Dif_L2</th>\n",
       "      <th>Dif_L3</th>\n",
       "      <th>Dif_L4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uniform</td>\n",
       "      <td>high</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>23.3</td>\n",
       "      <td>24.7</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>uniform</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>33.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>uniform</td>\n",
       "      <td>low</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>16.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non-uni-balanced</td>\n",
       "      <td>high</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>28.8</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-uni-balanced</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>29.2</td>\n",
       "      <td>20.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>29.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-uni-balanced</td>\n",
       "      <td>low</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>38.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>11.7</td>\n",
       "      <td>38.7</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.7</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>skewed-A</td>\n",
       "      <td>high</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>28.6</td>\n",
       "      <td>24.7</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skewed-A</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>24.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skewed-A</td>\n",
       "      <td>low</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.3</td>\n",
       "      <td>10.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>12.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>skewed-B</td>\n",
       "      <td>high</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>34.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>skewed-B</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>39.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>25.6</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>skewed-B</td>\n",
       "      <td>low</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>12.5</td>\n",
       "      <td>44.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>37.5</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dist    corr  rho  correlation  iter  per_L1  per_L2  per_L3  \\\n",
       "9            uniform    high  0.8          0.8   4.5    25.0    25.0    25.0   \n",
       "11           uniform  medium  0.6          0.5   4.5    25.0    25.0    25.0   \n",
       "10           uniform     low  0.3          0.3   4.5    25.0    25.0    25.0   \n",
       "0   non-uni-balanced    high  0.8          0.7   4.5    15.0    35.0    35.0   \n",
       "2   non-uni-balanced  medium  0.6          0.5   4.5    15.0    35.0    35.0   \n",
       "1   non-uni-balanced     low  0.3          0.3   4.5    15.0    35.0    35.0   \n",
       "3           skewed-A    high  0.8          0.7   4.5    30.0    30.0    30.0   \n",
       "5           skewed-A  medium  0.6          0.5   4.5    30.0    30.0    30.0   \n",
       "4           skewed-A     low  0.3          0.3   4.5    30.0    30.0    30.0   \n",
       "6           skewed-B    high  0.8          0.7   4.5     5.0    15.0    50.0   \n",
       "8           skewed-B  medium  0.6          0.5   4.5     5.0    15.0    50.0   \n",
       "7           skewed-B     low  0.3          0.3   4.5     5.0    15.0    50.0   \n",
       "\n",
       "    per_L4    L1    L2    L3    L4  Dif_L1  Dif_L2  Dif_L3  Dif_L4  \n",
       "9     25.0  26.2  23.3  24.7  25.8     1.2     1.7     0.3     0.8  \n",
       "11    25.0  32.9  17.0  16.8  33.2     7.9     8.0     8.2     8.2  \n",
       "10    25.0  40.7   8.9   9.4  41.0    15.7    16.1    15.6    16.0  \n",
       "0     15.0  20.5  28.8  30.0  20.7     5.5     6.2     5.0     5.7  \n",
       "2     15.0  29.2  20.7  20.8  29.4    14.2    14.3    14.2    14.4  \n",
       "1     15.0  38.3  11.3  11.7  38.7    23.3    23.7    23.3    23.7  \n",
       "3     10.0  30.3  28.6  24.7  16.4     0.3     1.4     5.3     6.4  \n",
       "5     10.0  35.7  20.0  19.4  24.9     5.7    10.0    10.6    14.9  \n",
       "4     10.0  42.3  10.9  11.0  35.8    12.3    19.1    19.0    25.8  \n",
       "6     30.0  10.0  20.0  35.8  34.2     5.0     5.0    14.2     4.2  \n",
       "8     30.0  18.7  17.7  24.4  39.2    13.7     2.7    25.6     9.2  \n",
       "7     30.0  31.0  12.3  12.5  44.2    26.0     2.7    37.5    14.2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199d281-9435-4525-b92b-84ba9a4b0331",
   "metadata": {},
   "source": [
    "## c. Cut Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "05305b1c-16ef-4bcf-9bf0-ecfa3c1f1191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cut_table = cut_tables.groupby([ 'dist','corr']).mean()\n",
    "cut_table = cut_table.reset_index()\n",
    "\n",
    "# Convert the column 'dist' to categorical with specified order\n",
    "cut_table['dist'] = pd.Categorical(cut_table['dist'], categories=specified_order_dist, ordered=True)\n",
    "cut_table['corr'] = pd.Categorical(cut_table['corr'], categories=specified_order_corr, ordered=True)\n",
    "# Sort the DataFrame by the 'dist' column\n",
    "cut_table = cut_table.sort_values(by=['dist','corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e476c291-2b7e-4b31-ad3d-6c9e5c6535cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cut_table.round(2).to_excel(r'C:\\Users\\jichen\\Documents\\Cambium\\ESS_criteria\\2024' + '\\\\'+method+'\\\\cut_table_'+\\\n",
    "                            method+'_Full.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7c92db92-b278-4130-9e62-de77ca7bfb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cut_tables.to_csv(r'C:\\Users\\jichen\\Documents\\Cambium\\ESS_criteria\\2024' + '\\\\'+method+'\\\\cut_table_'+\\\n",
    "                            method+'_by_iterations.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e04c5928-454c-4815-bc92-7711988fb276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_bootstrap_means(df, sample_size = 10, n_samples =1000): # n_samples is the number of bootstrap samples, \n",
    "    #sample_size is the sample size of each sample\n",
    "    # Array to store bootstrap means\n",
    "    bootstrap_means = np.zeros(n_samples)\n",
    "    # Bootstrap resampling\n",
    "    for i in range(n_samples):\n",
    "        # Generate bootstrap sample\n",
    "        bootstrap_sample = np.random.choice(df, size = sample_size, replace=True)\n",
    "        # Calculate mean of bootstrap sample\n",
    "        bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "    return bootstrap_means\n",
    "\n",
    "# Method 2 Calculate the confidence interval\n",
    "# std = np.std(bootstrap_means, ddof=1)\n",
    "# mean = np.mean(bootstrap_means)\n",
    "# margin_of_error = std / np.sqrt(n) * np.abs(norm.ppf((1 - confidence_level) / 2))\n",
    "# ci_low = mean - margin_of_error\n",
    "# ci_up = mean + margin_of_error\n",
    "# confidence_interval = [ci_low, ci_up]\n",
    "\n",
    "# print(\"Bootstrap Confidence Interval (95%):\", confidence_interval_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95e0d764-a68e-41d3-95b3-cff87c0b3287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuts = cut_tables.loc[(cut_tables.dist == 'non-uni-balanced') & (cut_tables['corr'] =='high'),:].cut1\n",
    "tcuts = cut_tables.loc[(cut_tables.dist == 'non-uni-balanced') & (cut_tables['corr'] =='high'),:].true_cut1\n",
    "sample_size = 10\n",
    "n_samples = 1000\n",
    "\n",
    "def ci_overlap(cuts, tcuts): \n",
    "    confidence_interval_cut = np.percentile(gen_bootstrap_means(cuts,sample_size,n_samples), [2.5, 97.5])\n",
    "    confidence_interval_tcut = np.percentile(gen_bootstrap_means(tcuts,sample_size,n_samples), [2.5, 97.5])\n",
    "    max_L = max(confidence_interval_cut[0],confidence_interval_tcut[0])\n",
    "    min_U = min(confidence_interval_cut[1],confidence_interval_tcut[1])\n",
    "    if max_L > min_U:\n",
    "        overlap = 0\n",
    "    else:\n",
    "        overlap = min_U - max_L\n",
    "    print(\"Bootstrap Confidence Interval (95%):\", confidence_interval_cut)\n",
    "    print(\"Bootstrap Confidence Interval (95%):\", confidence_interval_tcut)\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b8ee6b8c-d2c2-4c24-a803-1ca657923745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95%): [-1.412    -1.214975]\n",
      "Bootstrap Confidence Interval (95%): [-1.063 -1.014]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_overlap(cuts,tcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1e82d7bc-1384-4a29-86ac-1351c885ae71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddbe1a-ada5-42f7-ab24-db74350c6144",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29d189-3bd8-4635-ad46-abe8e006c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_table(df_list): \n",
    "    table = []\n",
    "    for df_i in df_list:\n",
    "        marg = (df_i.thetaScore.var()-((df_i.thetaSE)**2).mean())/df_i.thetaScore.var()\n",
    "        table.append((df_i.name,marg))\n",
    "    table = pd.DataFrame(table).rename(columns={0:'test',1:'marginal reliability'})\n",
    "    return (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d37bb-5b5d-4276-a683-8c02d3a2a940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea2ad5a4-7dd3-4650-af9b-5286346ce568",
   "metadata": {},
   "source": [
    "## d. Crosstab tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "af5b370f-7261-49f5-9289-3117934d9137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crosstab_mean = crosstab_tables.groupby(['dist','corr','ALDs']).mean().reset_index()\n",
    "pivot_table = crosstab_mean.pivot_table(index=['dist', 'ALDs'], columns='corr').reset_index()\n",
    "# Define the pre-specified order\n",
    "specified_order = ['uniform', 'non-uni-balanced', 'skewed-A','skewed-B', 'skewed-C', 'skewed-D']\n",
    "\n",
    "# Convert the column 'dist' to categorical with specified order\n",
    "pivot_table['dist'] = pd.Categorical(pivot_table['dist'], categories=specified_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by the 'dist' column\n",
    "pivot_table = pivot_table.sort_values(by=['dist','ALDs'])\n",
    "# # Rename the columns to include 'corr' information\n",
    "pivot_table.columns = ['{}_{}'.format(col[1], col[0]) for col in pivot_table.columns]\n",
    "\n",
    "# Sort the columns\n",
    "#sorted_columns = sorted(pivot_table.columns, key=lambda x: (x.split('_')[0], x.split('_')[1]))\n",
    "sorted_columns = ['_dist','_ALDs','high_1.0','high_2.0','high_3.0','high_4.0','high_Total',\n",
    " 'medium_1.0','medium_2.0','medium_3.0','medium_4.0','medium_Total', \n",
    "'low_1.0','low_2.0','low_3.0','low_4.0','low_Total']\n",
    "pivot_table = pivot_table[sorted_columns]\n",
    "pivot_table.to_excel(output_dir + '\\\\'+method+'\\\\crosstab_'+method+'.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d52b61-fec3-4cf1-90f8-f5c4032b3440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59274002-c7c7-4b07-8710-481250be3c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1116394-ce82-4cf0-99d2-8f9cdc06514d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
